{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1><strong> Twitter Sentiment Analysis using ML </strong></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kindn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset have been read.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\kindn\\OneDrive\\Documents\\dataset.csv\",encoding='ISO-8859-1')\n",
    "print(\"Dataset have been read.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1467810369</th>\n",
       "      <th>Mon Apr 06 22:19:45 PDT 2009</th>\n",
       "      <th>NO_QUERY</th>\n",
       "      <th>_TheSpecialOne_</th>\n",
       "      <th>@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY _TheSpecialOne_  \\\n",
       "0  0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   scotthamilton   \n",
       "1  0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY        mattycus   \n",
       "2  0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY         ElleCTF   \n",
       "3  0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          Karoli   \n",
       "4  0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY        joy_wolf   \n",
       "\n",
       "  @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  \n",
       "0  is upset that he can't update his Facebook by ...                                                                   \n",
       "1  @Kenichan I dived many times for the ball. Man...                                                                   \n",
       "2    my whole body feels itchy and like its on fire                                                                    \n",
       "3  @nationwideclass no, it's not behaving at all....                                                                   \n",
       "4                      @Kwesidei not the whole crew                                                                    "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the number of rows and columns df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1048575, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset with column names have been read.\n"
     ]
    }
   ],
   "source": [
    "# naming the cols and reading the dataset again\n",
    "col_names = ['target','id','date','flag','user','text']\n",
    "df = pd.read_csv(r\"C:\\Users\\kindn\\OneDrive\\Documents\\dataset.csv\", names=col_names, encoding='ISO-8859-1')\n",
    "print(\"Dataset with column names have been read.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1048576, 6)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target    0\n",
       "id        0\n",
       "date      0\n",
       "flag      0\n",
       "user      0\n",
       "text      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding the missing values in the dataset\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code> There are no missing values in the dataset. </code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    800000\n",
       "4    248576\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# finding the distribution of \"target\" column\n",
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our own convenience we can change Label \"4\" into \"1\". 1 means positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    800000\n",
       "1    248576\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.replace({'target':{4:1}}, inplace=True)\n",
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code> Now, <br> 0 --> Negative Tweet <br> 1 --> Positive Tweet </code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_negative=df[df['target'] == 0]\n",
    "df_positive=df[df['target'] == 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_positive_unsampled = resample(df_positive,replace=True,n_samples=len(df_negative),random_state=42)\n",
    "df_balanced = pd.concat([df_negative,df_positive_unsampled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    800000\n",
       "1    800000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df_balanced\n",
    "df['target'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative=df[df['target'] == 0]\n",
    "positive=df[df['target'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_sample=positive.sample(n=900,random_state=25)\n",
    "negative_sample=negative.sample(n=900,random_state=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df=pd.concat([positive_sample,negative_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_df=sampled_df.sample(frac=1,random_state=1).reset_index(drop=True)\n",
    "df=sampled_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    900\n",
       "1    900\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2222442595</td>\n",
       "      <td>Thu Jun 18 06:59:28 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>vortechs2000</td>\n",
       "      <td>@kpreslan I would have if I had an iPhone....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1833367298</td>\n",
       "      <td>Sun May 17 23:04:04 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Eunmi_Jeong</td>\n",
       "      <td>Today is monday. The Seoul in Korea is......ve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1835563314</td>\n",
       "      <td>Mon May 18 06:16:23 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>theclarkster</td>\n",
       "      <td>@MrsClarkster Thank you  xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1574005963</td>\n",
       "      <td>Tue Apr 21 03:51:38 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Marneezy</td>\n",
       "      <td>@FFEJyourself LOL. I love how its almost 4 a.m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1968170692</td>\n",
       "      <td>Fri May 29 20:53:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>PinkerJewel</td>\n",
       "      <td>@hollybird Aww.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                          date      flag          user  \\\n",
       "0       0  2222442595  Thu Jun 18 06:59:28 PDT 2009  NO_QUERY  vortechs2000   \n",
       "1       1  1833367298  Sun May 17 23:04:04 PDT 2009  NO_QUERY   Eunmi_Jeong   \n",
       "2       1  1835563314  Mon May 18 06:16:23 PDT 2009  NO_QUERY  theclarkster   \n",
       "3       0  1574005963  Tue Apr 21 03:51:38 PDT 2009  NO_QUERY      Marneezy   \n",
       "4       0  1968170692  Fri May 29 20:53:53 PDT 2009  NO_QUERY   PinkerJewel   \n",
       "\n",
       "                                                text  \n",
       "0     @kpreslan I would have if I had an iPhone....   \n",
       "1  Today is monday. The Seoul in Korea is......ve...  \n",
       "2                       @MrsClarkster Thank you  xxx  \n",
       "3  @FFEJyourself LOL. I love how its almost 4 a.m...  \n",
       "4                                   @hollybird Aww.   "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>STEMMING:</code> The process of reducing a word to it's root word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e.g: diving, diver, dived == dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PorterStemmer>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "port_stem = PorterStemmer()\n",
    "port_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(content):\n",
    "    stemmed_content = re.sub('[^a-zA-Z]', ' ', content)\n",
    "    stemmed_content = stemmed_content.lower()\n",
    "    stemmed_content = stemmed_content.split()\n",
    "    stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]\n",
    "    stemmed_content = ' '.join(stemmed_content)\n",
    "    \n",
    "    return stemmed_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['stemmed_content'] = df['text'].apply(stemming)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>stemmed_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2222442595</td>\n",
       "      <td>Thu Jun 18 06:59:28 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>vortechs2000</td>\n",
       "      <td>@kpreslan I would have if I had an iPhone....</td>\n",
       "      <td>kpreslan would iphon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1833367298</td>\n",
       "      <td>Sun May 17 23:04:04 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Eunmi_Jeong</td>\n",
       "      <td>Today is monday. The Seoul in Korea is......ve...</td>\n",
       "      <td>today monday seoul korea sunni day happi go st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1835563314</td>\n",
       "      <td>Mon May 18 06:16:23 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>theclarkster</td>\n",
       "      <td>@MrsClarkster Thank you  xxx</td>\n",
       "      <td>mrsclarkster thank xxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1574005963</td>\n",
       "      <td>Tue Apr 21 03:51:38 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Marneezy</td>\n",
       "      <td>@FFEJyourself LOL. I love how its almost 4 a.m...</td>\n",
       "      <td>ffejyourself lol love almost time think went b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1968170692</td>\n",
       "      <td>Fri May 29 20:53:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>PinkerJewel</td>\n",
       "      <td>@hollybird Aww.</td>\n",
       "      <td>hollybird aww</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                          date      flag          user  \\\n",
       "0       0  2222442595  Thu Jun 18 06:59:28 PDT 2009  NO_QUERY  vortechs2000   \n",
       "1       1  1833367298  Sun May 17 23:04:04 PDT 2009  NO_QUERY   Eunmi_Jeong   \n",
       "2       1  1835563314  Mon May 18 06:16:23 PDT 2009  NO_QUERY  theclarkster   \n",
       "3       0  1574005963  Tue Apr 21 03:51:38 PDT 2009  NO_QUERY      Marneezy   \n",
       "4       0  1968170692  Fri May 29 20:53:53 PDT 2009  NO_QUERY   PinkerJewel   \n",
       "\n",
       "                                                text  \\\n",
       "0     @kpreslan I would have if I had an iPhone....    \n",
       "1  Today is monday. The Seoul in Korea is......ve...   \n",
       "2                       @MrsClarkster Thank you  xxx   \n",
       "3  @FFEJyourself LOL. I love how its almost 4 a.m...   \n",
       "4                                   @hollybird Aww.    \n",
       "\n",
       "                                     stemmed_content  \n",
       "0                               kpreslan would iphon  \n",
       "1  today monday seoul korea sunni day happi go st...  \n",
       "2                             mrsclarkster thank xxx  \n",
       "3  ffejyourself lol love almost time think went b...  \n",
       "4                                      hollybird aww  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                    kpreslan would iphon\n",
      "1       today monday seoul korea sunni day happi go st...\n",
      "2                                  mrsclarkster thank xxx\n",
      "3       ffejyourself lol love almost time think went b...\n",
      "4                                           hollybird aww\n",
      "                              ...                        \n",
      "1795                                 sick wife bday doubl\n",
      "1796    saw possibl gh final band list frig slipknot s...\n",
      "1797                                                 cold\n",
      "1798                                happi birthday sister\n",
      "1799                                  final go sleep hour\n",
      "Name: stemmed_content, Length: 1800, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['stemmed_content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0\n",
      "1       1\n",
      "2       1\n",
      "3       0\n",
      "4       0\n",
      "       ..\n",
      "1795    0\n",
      "1796    0\n",
      "1797    0\n",
      "1798    1\n",
      "1799    0\n",
      "Name: target, Length: 1800, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperating stemmed data and label\n",
    "X = df['stemmed_content'].values\n",
    "Y = df['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kpreslan would iphon'\n",
      " 'today monday seoul korea sunni day happi go studi everyth nice day'\n",
      " 'mrsclarkster thank xxx' ... 'cold' 'happi birthday sister'\n",
      " 'final go sleep hour']\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 ... 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Data into Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2, stratify=Y, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1800,) (1440,) (360,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape,X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['djbeatbean tristeza ahshau' 'http twitpic com fatp poor car hit deer'\n",
      " 'happi miss sleepi love' ...\n",
      " 'oh ps sinc alreadi probabl say quot happi mother day quot mom mother'\n",
      " 'monipython awh see twitter phone set get messag'\n",
      " 'jordanknight realli hope check post new one sometim soon http bit ly b']\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['olegsmith go go go go go amaz friend go laugh good eat mr smith'\n",
      " 'shaundiviney foot feel ur arm dw kiss betr u like doubt u wanna kiss foot hey ne thing possibl'\n",
      " 'dannymcfli get feel like redhead guess there point tri amp far away'\n",
      " 'dubaysin yeah bad nvr happen sigh' 'dannygokey feelin mutual danni boy'\n",
      " 'ugh feel horribl sick' 'mszjoycii yeah cinema play screen n cant wait c'\n",
      " 'thesongsiwrot thought due tomorrow know wrong'\n",
      " 'terrytokyo wish could seen girl today well besid across crowd haah hope fun'\n",
      " 'nicolejensen welcom' 'last day internet net whole week lol'\n",
      " 'pool awesom two hour got home took shower went mcdonald stomach hurt'\n",
      " 'booo work comput say perezhilton websit r rate im allow look sure littl peek wouldnt hurt'\n",
      " 'celebr papa day work tomorrow suppos rain day'\n",
      " 'thepistol http twitpic com cb alex love u hah bad ur come sf anymor wait til u guy come back'\n",
      " 'booo ee suck daniella bring sam back happi'\n",
      " 'christinecain run chri run good c u yest great day go bac holland avo pump xoxo'\n",
      " 'fuck realiz cs project week fuck life'\n",
      " 'evaruth yeah almost like xena warrior princess'\n",
      " 'jaonyourmind caresslepor quot shoot way paper bag quot one day may plastic'\n",
      " 'could peopl pleas mourn fir lizard love die'\n",
      " 'hate life realiz total confus week june realiz week left sum school'\n",
      " 'efficialdudley suppos go backstag friend joey know someth got mess'\n",
      " 'tire hell bed time cara nighti night twitter world' 'phone die'\n",
      " 'queermonkey good morn morn'\n",
      " 'tire mlm get rich scam read http bit ly cqk g spread word like'\n",
      " 'ok back work although rather go univers today compani degre weather talk hero'\n",
      " 'litalea mean copyright kno wut forget im go bed shyt wasnt cool twigger im freak jealou'\n",
      " 'sessytav upset h amp sc suppos get th shirt'\n",
      " 'thank good internet back thunderstorm knock'\n",
      " 'eu verg enza todo lo que votaron shame use vote'\n",
      " 'omg allergi stay home laundri wanna kiss leoooo'\n",
      " 'kpaulin oh gt lt exactli happen'\n",
      " 'hinkybink yea last wk food rain far ugh hate fkn rain'\n",
      " 'paperworksssss even know start'\n",
      " 'realli realli angri aslo tire run sheep wield knife sane way start day'\n",
      " 'audaciousgloop hm usual stick one format time mayb get chanc serious experi fun'\n",
      " 'shanna yup' 'uhm swim tomorrow swim mile pool'\n",
      " 'eyelash though hurrah organis rais money akt'\n",
      " 'need good vibe pass test tomorrow eeeeek' 'chemgk hope realli great one'\n",
      " 'scottyorang kid b day parti best fun happi b day'\n",
      " 'osmentemili awh thank bianca holland holand sophi xd holland amaz thank shoutout'\n",
      " 'figur im awak'\n",
      " 'powertripp bagal ko pa rin mother still two round straight'\n",
      " 'yay problem fix lol haha'\n",
      " 'hope sick realli big project school want get left behind'\n",
      " 'aileenwilliam ye sky plu bgt turn' 'xn rdc r heard'\n",
      " 'toy dog brighton pier saw nick cave shop brown suit'\n",
      " 'sun shine fog gone look like love day'\n",
      " 'gwalshington fabul safe im look forward roommat soon'\n",
      " 'celebr rose tattoo year old' 'woah thought gonna cat fight'\n",
      " 'finish watch quot role model quot hilari debat whether go hmmm'\n",
      " 'today monday seoul korea sunni day happi go studi everyth nice day'\n",
      " 'reflect quot shaft quot may littl unwis'\n",
      " 'supercooperstar edam come wheel come ball damnit woman ball retain chees wench titl'\n",
      " 'turn recent increas blush cover face also shoulder chest even cleavag look embarass'\n",
      " 'phone famili man home sick awwwwwwww man suck'\n",
      " 'beauti sunni california morn' 'new day' 'civicbab haha kinda push'\n",
      " 'donnaspeak open tweetdeck amp saw tweet got alot ground ty g mornin friend'\n",
      " 'ok tell moma n pa vote co young'\n",
      " 'minut sit bed beauti kid sing row row row ur boat half asleep'\n",
      " 'disabl welcom rudi' 'come supermarket havent come sg yet'\n",
      " 'http twitpic com ryxu oh gay'\n",
      " 'degre c beaut day perfect long session front comput process photo'\n",
      " 'rachyyfac true yeah gambl hahah'\n",
      " 'friday night taco night yeah alreadi dinner like late lunch yay taco'\n",
      " 'prizegiv tonight wish summer would hurri serious'\n",
      " 'think tonight cuz tomorrow stuck work till' 'hoptonhousebnb email karen'\n",
      " 'sold car feel like fuck hobo' 'happi birthday sister'\n",
      " 'duhal okay guy rexft account login die'\n",
      " 'raconteurr haha watch realli excit' 'friend yesterday gosh much fun'\n",
      " 'foreverivi tv show csi heheh atleast still afternoon get sleep'\n",
      " 'bath troll care love omegl see happen usual seem end awe struck unabl form new offens'\n",
      " 'wish' 'msmoss sorri blog'\n",
      " 'mommykin http twitpic com yh awww cute happi mother day'\n",
      " 'guess spring go get eventu meantim penfabul morn dj http blip fm xl'\n",
      " 'slow internet work'\n",
      " 'wind umbrella speedlight broken wireless receiv alway weigh gear outdoor'\n",
      " 'treeseprais summer realli good wish sun would show face though amp sadli hope soon though haha'\n",
      " 'kcsroom worri spelt disappoint correctli look wrong'\n",
      " 'broke wuhzzup yall need sum cheerinq'\n",
      " 'reason live australia cont w music clock melbourn central x peter alexand spick amp speck z live'\n",
      " 'kallmebubbl thank never one word' 'kcdc thank hope use member'\n",
      " 'oh realli pm jimmi bennett still ad myspac' 'want comput back'\n",
      " 'fashionguru think like idea oompa loompa instead quot regular quot midget nod head ye mine'\n",
      " 'divinestorm unfortun' 'malou aww glad surgeri went well sorri pain'\n",
      " 'upset run vicodin'\n",
      " 'terribl back sprain yesterday morn bed ridden whole day'\n",
      " 'stupid twitter leav log let custom page way want'\n",
      " 'realwizkhalifa wonder sledgren ever some groupi even wouldnt say twitter thatd gangsta'\n",
      " 'heat remind katrina summer hella hot' 'listen cardigan'\n",
      " 'ball two gig cancel space hour fuck sake august truli go suck year booooooooooooo'\n",
      " 'thehomieb hahah yeah didnt'\n",
      " 'louis philp love song thought know word except cake rain part'\n",
      " 'selenagomez selli beg pleas replay make day pleas pleas love sooooooo much'\n",
      " 'senjohnmccain still think elect' 'watch titan babysit alon one talk'\n",
      " 'quanti cream cafe amp bar thur commun offic offic work'\n",
      " 'mia caus use want'\n",
      " 'wonder hell peopl freak flip flop scamper wet rocki dirt trail luggag ninja skill l'\n",
      " 'realli start math home work' 'adlanti clean'\n",
      " 'play sooth cranki patient oh give us peac good day http blip fm rkzu'\n",
      " 'super fun day dak anoth crazi show night store'\n",
      " 'tweet tweet oh babi mourn dove miss'\n",
      " 'kmosol sad face thought chicago lucki day come back home'\n",
      " 'usher awhh look share tell truth' 'sponehead love supris visit'\n",
      " 'chill pool work present doc relax friday'\n",
      " 'purelaura boy amp dad go beauti place need studi boo exam amp deadlin next week eek'\n",
      " 'come guy ask help orkut someon need help'\n",
      " 'summer sana http plurk com p rmmd'\n",
      " 'north korea good hope abl resolv tension hope bad probabl'\n",
      " 'best mother day ever'\n",
      " 'juz took shower go footbal practic anoth long day'\n",
      " 'miss jonathan spotifi friend'\n",
      " 'need sort thing morn clear email bring bank holiday weekend'\n",
      " 'marcochan sad' 'comput expert around disappoint' 'work x'\n",
      " 'boooom dia happi poynterday ddd'\n",
      " 'kyle newman star trek fan anyway trekki everywher great movi man thought anoth one'\n",
      " 'wgme certainli hope peopl fallen pier'\n",
      " 'chrispalko http twitpic com ltp must someth xtra must good lol'\n",
      " 'work book realiz might cut entir chapter'\n",
      " 'hannahsuarez anthil come bi monthli' 'aaronmacn love make sure let know'\n",
      " 'iwant mtv award' 'kizwiz cours realli posit today'\n",
      " 'heva x nope seen lemar daniel lloyd boxer dude woman locat locat locat boy georg megan fox'\n",
      " 'officialcharic chase tell see video coz happiest girl aliv lol jk'\n",
      " 'know said bbq maitai go northshor instead' 'need dual layer dvd'\n",
      " 'arminvanbuuren read news wii game plan say asot'\n",
      " 'time rsvp mobilemonday amsterdam'\n",
      " 'im happi got spend afternoon boy hate weekend back work ill crapper barrel til'\n",
      " 'tip blow sharpen'\n",
      " 'honestli slept hour last night gonna late care pancak worth'\n",
      " 'ericadiaz bring dvd u bring grappl'\n",
      " 'hnnngh song upload goddammit imeem go littl bit faster'\n",
      " 'teamwinnipeg hi gerri give u followfriday rec actual follow want u miss thankg right person'\n",
      " 'corn chowder soup pizza ye' 'follow anduial bu go studi guess'\n",
      " 'andrewtaranto lmao thank andrew er mean hershey'\n",
      " 'nisann wow treasur trove'\n",
      " 'know prob tmi realiz hardcor pmsing feel crazi' 'wish could chat'\n",
      " 'huge trig test math today work tonight doubl bummer week school left'\n",
      " 'ineverywordisay sorri'\n",
      " 'tommcfli http twitpic com ed x hahaha love harri amp dougi face guy look smokin oaken xxx'\n",
      " 'electrician work stalk kinda scare'\n",
      " 'littl worn yesterday festiv bad expect' 'lulazoid vacat good far'\n",
      " 'brought charli home shelter hid behind sofa half hour lunch'\n",
      " 'got shower ugh took extrem hot shower extrem hot day'\n",
      " 'gonna pick bernal uniform later eww uniform next yearr'\n",
      " 'miss alreadi lt' 'jennw time check be' 'upset believ'\n",
      " 'spoonsi oh would attract wrong crowd nocoffeeorshoppingnekkid'\n",
      " 'ali sweeney pleas follow'\n",
      " 'es f interestign setup fed forc buy bond whic pressur stock gvnmt spin news rais stock left amp right hand'\n",
      " 'miss gig' 'imposs sing hiccup' 'hair frikin awsom today today haha'\n",
      " 'omfg believ miss supernatur'\n",
      " 'michaelsheen aw repli anyon age miss anyway promis pic suit'\n",
      " 'meet alop byer http plurk com p uhbcb' 'look forward work tomorrow'\n",
      " 'myria haha sure sing greas'\n",
      " 'fldestri heavi high school free time hand tri thank'\n",
      " 'whew long one head bed get readi anoth hustl hustl hustl'\n",
      " 'chele tweetup time boooo nxt time guess ill tri make soon nce booo u'\n",
      " 'oop fell asleep hammock fault soo comfi' 'mrsclarkster thank xxx'\n",
      " 'countdown done exam day resid leav day leav week birthday week bahama day wolverin origin day'\n",
      " 'sick anymor cough' 'chelseasm kill'\n",
      " 'hope bit warmer today guess beach tan'\n",
      " 'happycassi well specifi nearli gave heart attack'\n",
      " 'blah sat film firework later' 'mincash bellagip get itm chiplead'\n",
      " 'beauti nice day stuck insid'\n",
      " 'heard amaz clay cooper show whole row sang loud think start choir'\n",
      " 'ugh crap cracker bore mind miss besti'\n",
      " 'luck find accommod next weekend hmmm back ute might'\n",
      " 'henrywellshaken miss'\n",
      " 'shit anoth piggi bank start fill get quot unnecessari want gadget quot palmpr look promis wish iphon b right'\n",
      " 'bori wrong use delici max allow bookmark lot stuff review research later'\n",
      " 'iatwitt sure' 'life crazi right thank famili great friend'\n",
      " 'done drunken tao done dan marino wing head home rolanda beat sun'\n",
      " 'hello stalker dig'\n",
      " 'zeldman dude grew divorc parent know person seem like fine job'\n",
      " 'ktrayyy haha probabl realiz haha jk serious pretti much girl suck life'\n",
      " 'love infrarecord littl smoke effect cd burn' 'fontblog still'\n",
      " 'go home chang work cheer doubt stay twitter even' 'theginmil welcom'\n",
      " 'tigoras cant sleeeeep need u' 'jesser yay twitter welcom'\n",
      " 'actual white castl sound tasti right none close either mayb need make road trip wc rule'\n",
      " 'well close u' 'douglasi thank follow dougla'\n",
      " 'agentman unsual got cancel' 'sorri guy work close tonight'\n",
      " 'got home drove myselfff got licens yesterdayxd' 'shittt exam one week'\n",
      " 'prettypinkpearl chat room launch' 'work b seven til dawn danc'\n",
      " 'renesebastian switch gmail use love'\n",
      " 'left church lunch somewher mum lol' 'ju got er car accid'\n",
      " 'must learn dont read paper earli depress alreadi damn countri'\n",
      " 'majornelson download full' 'wish overthink thing much go long day'\n",
      " 'datatlantachick lmfao u leo u hate smdh'\n",
      " 'sensuallysecret bet pompey lose bet get cashback easi http tinyurl com csla'\n",
      " 'love battlefield sacrific must made' 'gregfairbank better get need fix'\n",
      " 'last cruiser mixer colleg career'\n",
      " 'travino realli greasi yummi food like junk food like'\n",
      " 'fiiiinaaalllyyi home drive day speed ticket torenti rain fun time'\n",
      " 'got beaten year old chessclub' 'hope get job daycar appli'\n",
      " 'nancyseeg hour minut actual th rd ah well sat nite crush mtg'\n",
      " 'hmmmm want food' 'nathanfillion fault bad luck sadli'\n",
      " 'may day march today awesom la migra hate us even haha funni insid'\n",
      " 'thing hate touchscreen phone screen fingerprint amp facial greas magnet hate dirti screen'\n",
      " 'k night hug sure'\n",
      " 'morn manag prepar kiwi dd eat whilst eye still half close sleep readi tea flyladi'\n",
      " 'hey everybodi new' 'sad yao season come rocket' 'mood got kill'\n",
      " 'damn physic work hi amp bye twitter' 'time pass fast wait time soon'\n",
      " 'got back walk around gorgeou neighborhood love roomi'\n",
      " 'belledam hey give girl credit tri'\n",
      " 'agentrayray haha know fun leav alon fell like actual talk star follow lol jk'\n",
      " 'set phaser fun head kelli'\n",
      " 'ada acara menarik lain key vip di channel v ttg straight yg dikasih task approach stranger crowd hmm'\n",
      " 'thing look job hunt front'\n",
      " 'libertydog ld might get wish move next day rain fun'\n",
      " 'hilari w wow invent tizer random drink sadli get lot free mint'\n",
      " 'naww poor allan mean' 'argi'\n",
      " 'even pm friday night amp consid go bed hate sick'\n",
      " 'research project gener go kinemat ue manual w c propuls peopl tetraplegia'\n",
      " 'barbtheaussi hey use tweetdeck interest'\n",
      " 'wanna twitter friend onto yet gonna work'\n",
      " 'astronick realli leav flat lookout lunch fanci wee stroll dunno oh well'\n",
      " 'xericalaraina u kno'\n",
      " 'jessieeeexox reason realli miss sorri repli yesterday time'\n",
      " 'everi followfriday peopl follow make like tweet sorri guy'\n",
      " 'ga begin guess travel much summer special trip'\n",
      " 'http twitpic com etkp pancho monkey trip hondura last month awww bless well cute want pet mon'\n",
      " 'go back work monday whhhhhhhyyyyi' 'ouch'\n",
      " 'oh ye happi nine year annivarsari hanson second studio album quot time around quot'\n",
      " 'im sad realli sad guy' 'cat wink know wink wink back'\n",
      " 'gandalfar view pleasur http longurl net ehkusu note might break browser'\n",
      " 'carlybarnsley would like go wilmingto soon pleas xx'\n",
      " 'go watch taylor drive'\n",
      " 'go work almost pm love graveyard shift got jami foxx cd today someth new listen nite'\n",
      " 'maxlago well doubl yay best way think start day'\n",
      " 'vicho see stand want tri sniff pura canci n trist con ust'\n",
      " 'readi weekend wednesday yike'\n",
      " 'goin mall caus friend think funni leav sad' 'beam orangeri phew'\n",
      " 'jonathannyc think way thing go better get use idea broken promis'\n",
      " 'drink tonight first time sinc new year gonna die lol tell mum love haha'\n",
      " 'larri jensen know fell tonight amp face bang pretti larri sigh'\n",
      " 'first weekend without'\n",
      " 'carolina hahahaha omg win internetz today quot tri turn zac efron quot hahaha'\n",
      " 'favourit pen broke'\n",
      " 'saw possibl gh final band list frig slipknot still gay scar broadway nirvana enough justifi purchas'\n",
      " 'kpreslan would iphon' 'happen os realli go come tomorrow wtf'\n",
      " 'color colorado sunni high' 'miser grey day work coupl hour'\n",
      " 'greggarbo http twitpic com sjsn see either' 'omg that lota follow'\n",
      " 'topsyyturvyy idea gosh wait meet poppi' 'got new phone yay'\n",
      " 'imeantheend like' 'long day school today tire guitar sure crank notch'\n",
      " 'ahh still sooo hot hope ac fix work today' 'sarinninja thank tuna salad'\n",
      " 'clair hooper awesom gnw' 'verabradley follow dm win'\n",
      " 'sweetest thing ever heard'\n",
      " 'twitmugshot hmmmm biggest crush gotta mr trey songz'\n",
      " 'went fortun teller told chanc win squarespac competit dammit'\n",
      " 'go attempt essay french project'\n",
      " 'hmm receiv bad news horrid phone convo ever typic realli'\n",
      " 'millennialprof old one'\n",
      " 'deadpanalley still work tonight give anyth creat instead'\n",
      " 'rain pour love'\n",
      " 'giuliaboverio http twitpic com fs z tehe watch show youtub lol im america pretti good xx'\n",
      " 'mrjellybeanz morn' 'home realli bad migran feel well upset ughh'\n",
      " 'jonaseww caus rock adam' 'herbadgrandma point share whatev think time'\n",
      " 'ive arriv usa polic search took away food'\n",
      " 'http twitpic com v best time grooovin saturday girl'\n",
      " 'colettejan got megan dress instant faketan tenner tonight phone half'\n",
      " 'back work today' 'know possibl bore ice cream'\n",
      " 'palalond yeah thought wait cool applic avail twitter app group twitter'\n",
      " 'work woman top morn ya' 'defeat hurt'\n",
      " 'go danc nice day want stay outsid'\n",
      " 'go back eclips ide netbean bitch love'\n",
      " 'alexalltimelow thank tweet lt wish could someth help realli'\n",
      " 'would rather sit watch movi stupid homework xxx'\n",
      " 'order gs onlin wait arriv'\n",
      " 'nkotblorib last week learn sometim fate need interven balanc thing readi'\n",
      " 'theori weekend write chat gonna fabul'\n",
      " 'back la back work tomorrow wanna watch night museum'\n",
      " 'jesu time trackbal amp menu button work area phone tonight suck told guess sleep'\n",
      " 'work http twerpscan com' 'think caffein headach go make head explod'\n",
      " 'good morn got back earli morn meet lot today' 'love mommi'\n",
      " 'kluless found telli fone' 'lucascruikshank u make fred video pleaas'\n",
      " 'need updat phone ubertwitt work' 'babe go work day'\n",
      " 'bekkyxo know want go back exam thrill stuff x' 'prepar exam hour go'\n",
      " 'thank mr aditkok'\n",
      " 'two hour wait next class start boo haha obra castro cafe'\n",
      " 'eat bacon egg chees http anthonytriolo com'\n",
      " 'happi mother day mama great appreci mama'\n",
      " 'misssmith ok evonn amp henri take care financi'\n",
      " 'mother law im lock hous bad tho kind make sandwich'\n",
      " 'got new bikini today pink amp white'\n",
      " 'latest updat san fabric team go well take way long sound like today updat hubbl mission'\n",
      " 'wide receiv hate least twitterfriend get us day though'\n",
      " 'went go get eyebrow thread show ladi id last year goe happen pretti gt ego damag'\n",
      " 'road hour ahead beauti girl behind'\n",
      " 'guy india work deliv babi today stillborn sad time']\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As ML algorithms do not recognize textual data, so we have to convert them into numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 66)\t0.5773502691896257\n",
      "  (0, 3430)\t0.5773502691896257\n",
      "  (0, 869)\t0.5773502691896257\n",
      "  (1, 803)\t0.44495336269235797\n",
      "  (1, 1476)\t0.3635759511340483\n",
      "  (1, 500)\t0.3566619227458396\n",
      "  (1, 2511)\t0.3350759655265712\n",
      "  (1, 1097)\t0.44495336269235797\n",
      "  (1, 635)\t0.27058589266373456\n",
      "  (1, 3467)\t0.3069243571674121\n",
      "  (1, 1533)\t0.25483842267781276\n",
      "  (2, 1966)\t0.4090710550016978\n",
      "  (2, 2975)\t0.641974050025241\n",
      "  (2, 2119)\t0.44022152246794305\n",
      "  (2, 1408)\t0.47616719984685374\n",
      "  (3, 776)\t0.38537976711657007\n",
      "  (3, 3432)\t0.414922393388852\n",
      "  (3, 1262)\t0.414922393388852\n",
      "  (3, 1267)\t0.21155184278406447\n",
      "  (3, 1947)\t0.414922393388852\n",
      "  (3, 3580)\t0.28371740508554666\n",
      "  (3, 2481)\t0.32184361945235995\n",
      "  (3, 544)\t0.325833690668373\n",
      "  (4, 1931)\t0.49415919579988393\n",
      "  (4, 387)\t0.5220865582240161\n",
      "  :\t:\n",
      "  (1437, 2945)\t0.2577489781766171\n",
      "  (1437, 2609)\t0.4313696627346539\n",
      "  (1437, 2322)\t0.20699835370064468\n",
      "  (1437, 781)\t0.16256850378903628\n",
      "  (1437, 1408)\t0.21707431359415819\n",
      "  (1438, 230)\t0.4511096582961547\n",
      "  (1438, 2159)\t0.4511096582961547\n",
      "  (1438, 2082)\t0.3965779200973892\n",
      "  (1438, 2874)\t0.35532625262041345\n",
      "  (1438, 2451)\t0.35532625262041345\n",
      "  (1438, 3468)\t0.25387059730602757\n",
      "  (1438, 2843)\t0.2671506680278173\n",
      "  (1438, 1267)\t0.21769899969527193\n",
      "  (1439, 1719)\t0.39808944791452966\n",
      "  (1439, 559)\t0.31909714550509777\n",
      "  (1439, 2522)\t0.31909714550509777\n",
      "  (1439, 3023)\t0.33229587142250594\n",
      "  (1439, 2649)\t0.23118195380660525\n",
      "  (1439, 3031)\t0.27716049945810234\n",
      "  (1439, 2257)\t0.2420869188900644\n",
      "  (1439, 1512)\t0.2381948589735578\n",
      "  (1439, 2340)\t0.22903803266065118\n",
      "  (1439, 1981)\t0.3252829662555534\n",
      "  (1439, 336)\t0.26758519463609337\n",
      "  (1439, 1533)\t0.2279980229329378\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2192)\t0.2903582584070109\n",
      "  (0, 1859)\t0.26380648301332965\n",
      "  (0, 1307)\t0.1408888458197244\n",
      "  (0, 1298)\t0.82747269932311\n",
      "  (0, 1207)\t0.21317515221840921\n",
      "  (0, 954)\t0.21317515221840921\n",
      "  (0, 109)\t0.22505598247283545\n",
      "  (1, 3587)\t0.21563163507375394\n",
      "  (1, 3523)\t0.2455104317696265\n",
      "  (1, 3304)\t0.20725554375570943\n",
      "  (1, 2521)\t0.27298612616808393\n",
      "  (1, 1901)\t0.16126750217419497\n",
      "  (1, 1799)\t0.5459722523361679\n",
      "  (1, 1461)\t0.23288722901895947\n",
      "  (1, 1174)\t0.6009236411330827\n",
      "  (1, 1115)\t0.19055904297271162\n",
      "  (2, 3426)\t0.2806649461753161\n",
      "  (2, 3294)\t0.41045832620773526\n",
      "  (2, 2504)\t0.3939997819117998\n",
      "  (2, 1901)\t0.23275673961163454\n",
      "  (2, 1363)\t0.32656789329397123\n",
      "  (2, 1267)\t0.20927579874902472\n",
      "  (2, 1115)\t0.2750331030608531\n",
      "  (2, 1089)\t0.3619836706680804\n",
      "  (2, 228)\t0.34157798353563024\n",
      "  :\t:\n",
      "  (357, 3316)\t0.3363937138703973\n",
      "  (357, 2922)\t0.23648265018428938\n",
      "  (357, 2550)\t0.24697345118772907\n",
      "  (357, 1852)\t0.20572116820152855\n",
      "  (357, 1838)\t0.2957292465970144\n",
      "  (357, 1407)\t0.24410926792211493\n",
      "  (357, 1361)\t0.2957292465970144\n",
      "  (357, 1300)\t0.287637918461112\n",
      "  (357, 1298)\t0.15977768061360248\n",
      "  (357, 1267)\t0.1623387432890777\n",
      "  (357, 1065)\t0.3363937138703973\n",
      "  (357, 967)\t0.3363937138703973\n",
      "  (358, 2733)\t0.4739783738524791\n",
      "  (358, 1523)\t0.33658464934976423\n",
      "  (358, 1281)\t0.36193861722974036\n",
      "  (358, 288)\t0.448624405972503\n",
      "  (358, 273)\t0.3799275437211455\n",
      "  (358, 61)\t0.43063547948109776\n",
      "  (359, 3690)\t0.2837923084805569\n",
      "  (359, 3366)\t0.2910552847589363\n",
      "  (359, 3340)\t0.2868145312822308\n",
      "  (359, 2776)\t0.3413776936745759\n",
      "  (359, 1604)\t0.5382634260865724\n",
      "  (359, 1371)\t0.40008139359413875\n",
      "  (359, 238)\t0.43145660778955436\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training The ML Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_predictions = model.predict(X_train)\n",
    "training_data_accuracy = accuracy_score(Y_train, X_train_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the training data is 0.94375\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy on the training data is {training_data_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_predictions = model.predict(X_test)\n",
    "testing_data_accuracy = accuracy_score(Y_test, X_test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the testing data is 0.6944444444444444\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy on the testing data is {testing_data_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving The Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'trained_model.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_vectorizer = 'vectorizer.sav'\n",
    "pickle.dump(vectorizer, open(filename_vectorizer, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using The Saved Model For Future Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the saved model\n",
    "loaded_model = pickle.load(open('./trained_model.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[1]\n",
      "It is a positive tweet.\n"
     ]
    }
   ],
   "source": [
    "X_new = X_test[309]\n",
    "print(Y_test[309])\n",
    "prediction = model.predict(X_new)\n",
    "print(prediction)\n",
    "if (prediction[0] == 0):\n",
    "    print(\"It is a negative tweet.\")\n",
    "    \n",
    "else:\n",
    "    print(\"It is a positive tweet.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
